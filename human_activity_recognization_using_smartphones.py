# -*- coding: utf-8 -*-
"""Human Activity Recognization using SmartPhones.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LnU-IU4k4oou1aSoZk0hZehsfWm-vr65

#**HUMAN ACTIVITY RECOGNIZATION**

**Importing Packages**
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelBinarizer
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis

"""**Importing DataSet**"""

#import dataset
x = pd.read_csv('/content/train HAR.csv')
y = pd.read_csv('/content/test HAR.csv')

"""**EXPLORATORY DATA ANALYSIS**"""

#shape
print(f"Train Data Size: {x.shape}")
print(f"Test Data Size:{y.shape}")

x.dtypes

y.dtypes

x.columns
y.columns

x.head()

y.head()

x.info()

y.info()

"""# **STATS DESCRIPTION**"""

x.describe()

y.describe()

"""#**Checking Data Duplication**"""

x.duplicated().sum()

y.duplicated().sum()

x['Activity'].unique()

"""#**Checking Missing Value**"""

x.isnull().sum()

y.isnull().sum()

x.corr().style.background_gradient(cmap="coolwarm")

"""**Matplot library**"""

import matplotlib.pyplot as plt
plt.hist(x["Activity"])

"""# **Checking Label Data**"""

count = sns.countplot(x="Activity", data=X)
for bar in count.patches:
    count.annotate(format(bar.get_height()),
        (bar.get_x() + bar.get_width() / 2,
        bar.get_height()), ha='center', va='center',
        size=11, xytext=(0, 8),
        textcoords='offset points')
count.set_title("Activities Label Distribution")
plt.xticks(rotation=90)
plt.show()

X_train = x.drop(["subject", "Activity"], axis=1)
y_train = x.Activity

X_test = y.drop(["subject", "Activity"], axis=1)
y_test = y.Activity

"""**Dimension Reduction**"""

lda =LinearDiscriminantAnalysis(n_components=5)

X_train_lda= lda.fit_transform(X_train, y_train)
X_test_lda = lda.transform(X_test)
print(f"Data features before dimensionality reduction: {X_train.shape[1]}")
print(f"Data features after dimensionality reduction: {X_train_lda.shape[1]}")

pd.DataFrame(X_train_lda[:5])

"""**Lable Encoding**"""

lb = LabelBinarizer()

y_train_enc = lb.fit_transform(y_train)
y_test_enc = lb.transform(y_test)

"""**Building Classification Model using Extreme Learning Machine**"""

class ELM:
    def __init__(self, neuron_size, activation_function="relu", random_state=None):
        self.neuron_size = neuron_size
        self.activation_function = activation_function
        self.random_state = random_state

        np.seterr(divide="ignore", invalid="ignore")

    def fit(self, x_train, y_train):
        x_train = self.__is_numpy(x_train)
        self.weight = self.__get_weight(x_train, y_train)
        self.bias = self.__get_bias()

        H = self.__get_H(x_train)
        try :
            H_pinv = np.matmul(np.linalg.inv(np.matmul(H.T, H)), H.T)
        except :
            H_pinv = np.linalg.pinv(H)

        self.beta = np.matmul(H_pinv, y_train)

    def predict(self, x):
        x = self.__is_numpy(x)
        H = self.__get_H(x)
        self.y_awal = np.matmul(H, self.beta)

        return self.__get_y_onehot(self.y_awal)

    def __get_weight(self, x_train, y_train):
        np.random.seed(self.random_state)

        return np.random.uniform(-1, 1, size=(self.neuron_size, x_train.shape[1]))

    def __get_bias(self):
        np.random.seed(self.random_state)

        return np.random.uniform(-1, 1, size=self.neuron_size)

    def __activate(self, H):
        H_act = H.copy()

        if self.activation_function == "relu":
            relu = lambda x: np.maximum(x, 0)
            return relu(H_act)
        elif self.activation_function == "sigmoid":
            sigmoid = lambda x: 1 / (1 + np.exp(-x))
            return np.nan_to_num(sigmoid(H_act))
        elif self.activation_function == "tanh":
            tanh = lambda x: (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))
            return np.nan_to_num(tanh(H_act))
        elif self.activation_function == "sine":
            sine = lambda x: np.sin(x)
            return np.nan_to_num(sine(H_act))
        elif self.activation_function == "hardlim":
            hardlim = lambda x: np.where(x >= 0, 1, 0)
            return hardlim(H_act)

    def __get_H(self, x):
        H = np.matmul(x, self.weight.T) + self.bias

        return self.__activate(H)

    def __get_y_onehot(self, y_awal):
        y_onehot = np.zeros_like(y_awal, dtype="uint8")
        for i, y in enumerate(y_awal):
            y_onehot[i, np.argmax(y)] = 1

        return y_onehot

    def __is_numpy(self, x):
        if type(x) != "numpy.ndarray":
            return np.array(x)

elm = ELM(700, activation_function="relu",random_state=0)
elm.fit(X_train, y_train_enc)
y_pred = elm.predict(X_test)
print(f"Test accuracy using relu with original dataset: {accuracy_score(y_test_enc, y_pred)}\n")

elm = ELM(30, activation_function="relu", random_state=0)
elm.fit(X_train_lda, y_train_enc)
y_pred = elm.predict(X_test_lda)

print(f"Test accuracy using relu after dimensionality reduction: {accuracy_score(y_test_enc, y_pred)}\n")

print(classification_report(y_test_enc, y_pred))

elm = ELM(700, activation_function="tanh", random_state=0)
elm.fit(X_train, y_train_enc)
y_pred = elm.predict(X_test)
print(f"Test accuracy using tanh with original dataset: {accuracy_score(y_test_enc, y_pred)}\n")

elm = ELM(30, activation_function="tanh", random_state=0)
elm.fit(X_train_lda, y_train_enc)
y_pred = elm.predict(X_test_lda)
print(f"Test accuracy using tanh after dimensionality reduction: {accuracy_score(y_test_enc, y_pred)}\n")

print(classification_report(y_test_enc, y_pred))

elm = ELM(700, activation_function="sigmoid", random_state=0)
elm.fit(X_train, y_train_enc)
y_pred = elm.predict(X_test)
print(f"Test accuracy using sine with original dataset: {accuracy_score(y_test_enc, y_pred)}\n")

elm = ELM(30, activation_function="sigmoid", random_state=0)
elm.fit(X_train_lda, y_train_enc)
y_pred = elm.predict(X_test_lda)

print(f"Test accuracy using sine after dimensionality reduction: {accuracy_score(y_test_enc, y_pred)}\n")

print(classification_report(y_test_enc, y_pred))

elm = ELM(700, activation_function="sine", random_state=0)
elm.fit(X_train, y_train_enc)
y_pred = elm.predict(X_test)
print(f"Test accuracy using sine with original dataset: {accuracy_score(y_test_enc, y_pred)}\n")

elm = ELM(30, activation_function="sine", random_state=0)
elm.fit(X_train_lda, y_train_enc)
y_pred = elm.predict(X_test_lda)

print(f"Test accuracy using sine after dimensionality reduction: {accuracy_score(y_test_enc, y_pred)}\n")

print(classification_report(y_test_enc, y_pred))

elm = ELM(700, activation_function="hardlim",random_state=0)
elm.fit(X_train, y_train_enc)
y_pred = elm.predict(X_test)
print(f"Test accuracy using hardlim with original dataset: {accuracy_score(y_test_enc, y_pred)}\n")

elm = ELM(30, activation_function="hardlim", random_state=0)
elm.fit(X_train_lda, y_train_enc)
y_pred = elm.predict(X_test_lda)

print(f"Test accuracy using hardlim after dimensionality reduction: {accuracy_score(y_test_enc, y_pred)}\n")

print(classification_report(y_test_enc, y_pred))

"""From this I conclude that our ELM model uses LDA for dimensional reduction which helps us to reduce our features from 561 to 5
This no of features is very small compared to our original dataset.The model acquires higher accuracy when we extract the
features using LDA and we have applied various activation functions.RELU activation function is more efficient when
extracting the features compared to other types of activation function .The reduced Features requires more information than the
original features.Our ELM model get less accuracy with original features using 700 neurons, and high accuracy with reduced
features using 30 neurons.Therefore here comes huge difference in the no of neurons which helps the model training process
more faster.

"""